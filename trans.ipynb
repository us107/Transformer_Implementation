{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "16c17b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # for core funtionality , gives easy simple structure for NN , uses tensors as DATA STRUCTURES \n",
    "import torch.nn as nn # for neural networks\n",
    "import torch.optim # for training netwrokssss\n",
    "import math  # maths ops \n",
    "import copy # copying complex objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "638455b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5308ed",
   "metadata": {},
   "source": [
    "In summary, the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c74ebd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#position wise feed forward network\n",
    "\n",
    "class PositonWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff): #d_ff for dimension of feed forwrd layers\n",
    "        super(PositonWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu= nn.ReLU() # RELU ACTIVATION FUNCTION FOR NON L;INEARITY\n",
    "    def forward(self,x): # x for input to feed forward\n",
    "        return self.fc2(self.relu(self.fc1(x))) # firstly passed through first linear layer with input x and then passed through RELU activation function and then passed through 2nd linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f2813",
   "metadata": {},
   "source": [
    "## Encoding \n",
    "is a broader term for transforming data into a numerical representation, often for basic processing, while \n",
    "## Embedding \n",
    "is a more sophisticated technique that maps data into a vector space, capturing semantic relationships and similarities between data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b69258ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding for injecting position information of each token in the input sequence using sine and cosine funtions to generate positional encoding\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe= torch.zeros(max_seq_length, d_model) # A tensor filled with zeros, which will be populated with positional encodings\n",
    "        position= torch.arange(0,max_seq_length, dtype=torch.float).unsqueeze(1) #: A tensor containing the position indices for each position in the sequence.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) # used to scale the position indices in a specific way.\n",
    "\n",
    "        pe[:, 0::2]= torch.sin(position * div_term)\n",
    "        pe[:, 1::2]= torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe',pe.unsqueeze(0)) #he sine function is applied to the even indices and the cosine function to the odd indices of pe.\n",
    "#Finally, pe is registered as a buffer, which means it will be part of the module's state but will not be considered a trainable parameter.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)] #uses the first x.size(1) elements of pe to ensure that the positional encodings match the actual sequence length of x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef891685",
   "metadata": {},
   "source": [
    "## The PositionalEncoding \n",
    "class adds information about the position of tokens within the sequence. Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), this class helps the model to consider the position of tokens in the sequence. The sinusoidal functions used are chosen to allow the model to easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24d6014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn= MultiHeadAttention( d_model, num_heads)\n",
    "        self.feed_forward= PositonWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1= nn.LayerNorm(d_model) # layer normalisation to smooth the input\n",
    "        self.norm2= nn. LayerNorm(d_model)\n",
    "        self.dropout= nn.Dropout(dropout) #Dropout layer, used to prevent overfitting by randomly setting some activations to zero during training.\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output= self.self_attn(x,x,x, mask) # Optional mask to ignore certain parts of the input.\n",
    "        x= self.norm1(x + self.dropout(attn_output))   \n",
    "        ff_output= self.feed_forward(x)\n",
    "        x= self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28da1d8",
   "metadata": {},
   "source": [
    "## The EncoderLayer \n",
    "class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by the position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. Together, these components allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "84d2fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout): # dropout for regularization\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads) #Multi-head self-attention mechanism for the target sequence.\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads) # Multi-head attention mechanism that attends to the encoder's output.\n",
    "        self.feed_forward = PositonWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask): # x is input to the decoder layer\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)    \n",
    "        x= self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b8751",
   "metadata": {},
   "source": [
    "enc_output: The output from the corresponding encoder (used in the cross-attention step).\n",
    "src_mask: Source mask to ignore certain parts of the encoder's output.\n",
    "tgt_mask: Target mask to ignore certain parts of the decoder's input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd5b11",
   "metadata": {},
   "source": [
    "### The DecoderLayer \n",
    "class defines a single layer of the transformer's decoder. It consists of a multi-head self-attention mechanism, a multi-head cross-attention mechanism (that attends to the encoder's output), a position-wise feed-forward neural network, and the corresponding residual connections, layer normalization, and dropout layers. This combination enables the decoder to generate meaningful outputs based on the encoder's representations, taking into account both the target sequence and the source sequence. As with the encoder, multiple decoder layers are typically stacked to form the complete decoder part of a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "57b9ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the encoder and decoder layers to create the complete Transformer network\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length,  dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model) # Embedding layer for the source sequenc\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model) # Embedding layer for the target sequence.\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size) # final fully connected linear layer mapping to the target vocab\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#This method is used to create masks for the source and target sequences, ensuring that padding tokens are ignored and that future tokens are not visible during training for the target sequence.\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt): # taking source and target sequences and producing the output predictions.\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded # The source sequence is passed through the encoder layers, with the final encoder output representing the processed source sequence.\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded # The target sequence and the encoder's output are passed through the decoder layers, resulting in the decoder's output.\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "            output = self.fc(dec_output) # final fully connected output \n",
    "            return output        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88fa829",
   "metadata": {},
   "source": [
    "### the Transformer\n",
    " class brings together the various components of a Transformer model, including the embeddings, positional encoding, encoder layers, and decoder layers. It provides a convenient interface for training and inference, encapsulating the complexities of multi-head attention, feed-forward networks, and layer normalization.\n",
    "\n",
    "This implementation follows the standard Transformer architecture, making it suitable for sequence-to-sequence tasks like machine translation, text summarization, etc. Including masking ensures that the model adheres to the causal dependencies within sequences, ignoring padding tokens and preventing information leakage from future tokens.\n",
    "\n",
    "These sequential steps empower the Transformer model to efficiently process input sequences and produce corresponding output sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429f647",
   "metadata": {},
   "source": [
    "### why ignore padding tokens ?\n",
    "1st ans--> The model must learn to ignore `<pad>` tokens, which can add complexity to its training, as it must differentiate between two types of non-content-bearing tokens: one that signifies the end of content and one that is purely for maintaining consistent input sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa89f46",
   "metadata": {},
   "source": [
    "### Embedding means\n",
    "Embeddings are dense numeric vectors that capture semantic meaning for each token. They bring discrete language into a continuous vector space that transformers can analyze. The embeddings serve as the mathematical language and data representation that transformers operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b8af96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1755b",
   "metadata": {},
   "source": [
    "### Hyperparameter\tTypical values\tImpact on performance\n",
    "d_model\t256, 512, 1024\tHigher values increase model capacity but require more computation\n",
    "\n",
    "num_heads\t8, 12, 16\tMore heads can capture diverse aspects of data, but are computationally intensive\n",
    "\n",
    "num_layers\t6, 12, 24\tMore layers improve representation power, but can lead to overfitting\n",
    "\n",
    "d_ff\t2048, 4096\tLarger feed-forward networks increase model robustness\n",
    "\n",
    "dropout\t0.1, 0.3\tRegularizes the model to prevent overfitting\n",
    "\n",
    "learning rate\t0.0001 - 0.001\tImpacts convergence speed and stability\n",
    "\n",
    "batch size\t32, 64, 128\tLarger batch sizes improve learning stability but require more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295d512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.686553001403809\n",
      "Epoch: 2, Loss: 8.62042236328125\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim  # âœ… Add this line\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
