{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c17b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # for core funtionality , gives easy simple structure for NN , uses tensors as DATA STRUCTURES \n",
    "import torch.nn as nn # for neural networks\n",
    "import torch.optim # for training netwrokssss\n",
    "import math  # maths ops \n",
    "import copy # copying complex objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): # nn.Module is base class for all models \n",
    "    def __init__( self, d_model, num_heads): \n",
    "\n",
    "        # d_model===> model's dimensions and num_heads===> refer to multiple sets of learned linear transformations applied to the input query, key, and value vectors, enabling the model to capture different aspects of the input sequence in parallel. Each head independently performs attention calculations, and the outputs are then combined, typically through concatenation followed by another linear transformation. \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads==0 # assert is used to verify a condition is true or not in the code and here we want d_model(model's dimensions) to be divisible by the num_heads\n",
    "\n",
    "        # dimensions ko initialize karo\n",
    "\n",
    "        self.d_model= d_model\n",
    "        self.num_heads= num_heads\n",
    "        self.d_k= d_model // num_heads # DIMENSION OF each head's key, query and value\n",
    "\n",
    "\n",
    "        self.W_q= nn.Linear(d_model, d_model) # qyery transs\n",
    "        self.W_k= nn.Linear(d_model, d_model) # key \n",
    "        self.W_v= nn.Linear(d_model, d_model) # value\n",
    "        self.W_o= nn.Linear(d_model, d_model) # last mai output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled dot product attention which basically finds relation between input words\n",
    "\n",
    "def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "\n",
    "    attn_scores= torch.matmul(Q, K.transpose(-2,-1))/ math.sqrt(self.self.d_k) # matmul for Matrix product of two tensors.\n",
    "\n",
    "    if mask is not None:      # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "\n",
    "        attn_scores= attn_scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "    attn_probs= torch.softmax(attn_scores, dim=1)     # softmax for attention ko probabilities mai convert karne kelie that sums to 1 \n",
    "\n",
    "    output = torch.matmul(attn_probs, V)       # matmul final O/P\n",
    "    return output    \n",
    "\n",
    "\n",
    "\n",
    "def split_heads(self, x): #split kiya heads ko \n",
    "    batch_size, seq_length, d_model= x.size()\n",
    "    return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "\n",
    "def combine_heads(self,x): # yahan concat kiya multiple heads ka result\n",
    "\n",
    "    batch_size, _, seq_length, d_k= x.size()\n",
    "    return x.transpose(1,2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "def forward(self, Q, K, V, mask=None): # method applies linear transformations, splits the heads, performs scaled dot-product attention, combines the heads, and returns the output.\n",
    "    Q= self.split_heads(self.W_q(Q)) # splitting and multiplying with learnable weights\n",
    "    K= self.split_heads(self.W_k(K))\n",
    "    V= self.split_heads(self.W_v(V))\n",
    "\n",
    "    attn_output=self.scaled_dot_product_attention( Q, K, V, mask) # DOT PRODUCT CALL KIYA \n",
    "\n",
    "    output= self.W_o(self.combine_heads(attn_output))\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5308ed",
   "metadata": {},
   "source": [
    "In summary, the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ebd60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
