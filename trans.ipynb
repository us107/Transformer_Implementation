{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "16c17b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # for core funtionality , gives easy simple structure for NN , uses tensors as DATA STRUCTURES \n",
    "import torch.nn as nn # for neural networks\n",
    "import torch.optim # for training netwrokssss\n",
    "import math  # maths ops \n",
    "import copy # copying complex objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "638455b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5308ed",
   "metadata": {},
   "source": [
    "In summary, the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c74ebd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#position wise feed forward network\n",
    "\n",
    "class PositonWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff): #d_ff for dimension of feed forwrd layers\n",
    "        super(PositonWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu= nn.ReLU() # RELU ACTIVATION FUNCTION FOR NON L;INEARITY\n",
    "    def forward(self,x): # x for input to feed forward\n",
    "        return self.fc2(self.relu(self.fc1(x))) # firstly passed through first linear layer with input x and then passed through RELU activation function and then passed through 2nd linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f2813",
   "metadata": {},
   "source": [
    "## Encoding \n",
    "is a broader term for transforming data into a numerical representation, often for basic processing, while \n",
    "## Embedding \n",
    "is a more sophisticated technique that maps data into a vector space, capturing semantic relationships and similarities between data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b69258ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding for injecting position information of each token in the input sequence using sine and cosine funtions to generate positional encoding\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe= torch.zeros(max_seq_length, d_model) # A tensor filled with zeros, which will be populated with positional encodings\n",
    "        position= torch.arange(0,max_seq_length, dtype=torch.float).unsqueeze(1) #: A tensor containing the position indices for each position in the sequence.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) # used to scale the position indices in a specific way.\n",
    "\n",
    "        pe[:, 0::2]= torch.sin(position * div_term)\n",
    "        pe[:, 1::2]= torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe',pe.unsqueeze(0)) #he sine function is applied to the even indices and the cosine function to the odd indices of pe.\n",
    "#Finally, pe is registered as a buffer, which means it will be part of the module's state but will not be considered a trainable parameter.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)] #uses the first x.size(1) elements of pe to ensure that the positional encodings match the actual sequence length of x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef891685",
   "metadata": {},
   "source": [
    "## The PositionalEncoding \n",
    "class adds information about the position of tokens within the sequence. Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), this class helps the model to consider the position of tokens in the sequence. The sinusoidal functions used are chosen to allow the model to easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "24d6014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn= MultiHeadAttention( d_model, num_heads)\n",
    "        self.feed_forward= PositonWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1= nn.LayerNorm(d_model) # layer normalisation to smooth the input\n",
    "        self.norm2= nn. LayerNorm(d_model)\n",
    "        self.dropout= nn.Dropout(dropout) #Dropout layer, used to prevent overfitting by randomly setting some activations to zero during training.\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output= self.self_attn(x,x,x, mask) # Optional mask to ignore certain parts of the input.\n",
    "        x= self.norm1(x + self.dropout(attn_output))   \n",
    "        ff_output= self.feed_forward(x)\n",
    "        x= self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28da1d8",
   "metadata": {},
   "source": [
    "## The EncoderLayer \n",
    "class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by the position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. Together, these components allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "84d2fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout): # dropout for regularization\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads) #Multi-head self-attention mechanism for the target sequence.\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads) # Multi-head attention mechanism that attends to the encoder's output.\n",
    "        self.feed_forward = PositonWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask): # x is input to the decoder layer\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)    \n",
    "        x= self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b8751",
   "metadata": {},
   "source": [
    "enc_output: The output from the corresponding encoder (used in the cross-attention step).\n",
    "src_mask: Source mask to ignore certain parts of the encoder's output.\n",
    "tgt_mask: Target mask to ignore certain parts of the decoder's input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd5b11",
   "metadata": {},
   "source": [
    "### The DecoderLayer \n",
    "class defines a single layer of the transformer's decoder. It consists of a multi-head self-attention mechanism, a multi-head cross-attention mechanism (that attends to the encoder's output), a position-wise feed-forward neural network, and the corresponding residual connections, layer normalization, and dropout layers. This combination enables the decoder to generate meaningful outputs based on the encoder's representations, taking into account both the target sequence and the source sequence. As with the encoder, multiple decoder layers are typically stacked to form the complete decoder part of a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "57b9ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the encoder and decoder layers to create the complete Transformer network\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length,  dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model) # Embedding layer for the source sequenc\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model) # Embedding layer for the target sequence.\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size) # final fully connected linear layer mapping to the target vocab\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#This method is used to create masks for the source and target sequences, ensuring that padding tokens are ignored and that future tokens are not visible during training for the target sequence.\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt): # taking source and target sequences and producing the output predictions.\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded # The source sequence is passed through the encoder layers, with the final encoder output representing the processed source sequence.\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded # The target sequence and the encoder's output are passed through the decoder layers, resulting in the decoder's output.\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "            output = self.fc(dec_output) # final fully connected output \n",
    "            return output        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88fa829",
   "metadata": {},
   "source": [
    "### the Transformer\n",
    " class brings together the various components of a Transformer model, including the embeddings, positional encoding, encoder layers, and decoder layers. It provides a convenient interface for training and inference, encapsulating the complexities of multi-head attention, feed-forward networks, and layer normalization.\n",
    "\n",
    "This implementation follows the standard Transformer architecture, making it suitable for sequence-to-sequence tasks like machine translation, text summarization, etc. Including masking ensures that the model adheres to the causal dependencies within sequences, ignoring padding tokens and preventing information leakage from future tokens.\n",
    "\n",
    "These sequential steps empower the Transformer model to efficiently process input sequences and produce corresponding output sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429f647",
   "metadata": {},
   "source": [
    "### why ignore padding tokens ?\n",
    "1st ans--> The model must learn to ignore `<pad>` tokens, which can add complexity to its training, as it must differentiate between two types of non-content-bearing tokens: one that signifies the end of content and one that is purely for maintaining consistent input sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa89f46",
   "metadata": {},
   "source": [
    "### Embedding means\n",
    "Embeddings are dense numeric vectors that capture semantic meaning for each token. They bring discrete language into a continuous vector space that transformers can analyze. The embeddings serve as the mathematical language and data representation that transformers operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b8af96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1755b",
   "metadata": {},
   "source": [
    "### Hyperparameter\tTypical values\tImpact on performance\n",
    "d_model\t256, 512, 1024\tHigher values increase model capacity but require more computation\n",
    "\n",
    "num_heads\t8, 12, 16\tMore heads can capture diverse aspects of data, but are computationally intensive\n",
    "\n",
    "num_layers\t6, 12, 24\tMore layers improve representation power, but can lead to overfitting\n",
    "\n",
    "d_ff\t2048, 4096\tLarger feed-forward networks increase model robustness\n",
    "\n",
    "dropout\t0.1, 0.3\tRegularizes the model to prevent overfitting\n",
    "\n",
    "learning rate\t0.0001 - 0.001\tImpacts convergence speed and stability\n",
    "\n",
    "batch size\t32, 64, 128\tLarger batch sizes improve learning stability but require more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c295d512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.674612045288086\n",
      "Epoch: 2, Loss: 8.612031936645508\n",
      "Epoch: 3, Loss: 8.556350708007812\n",
      "Epoch: 4, Loss: 8.508870124816895\n",
      "Epoch: 5, Loss: 8.462989807128906\n",
      "Epoch: 6, Loss: 8.427068710327148\n",
      "Epoch: 7, Loss: 8.38417911529541\n",
      "Epoch: 8, Loss: 8.34548282623291\n",
      "Epoch: 9, Loss: 8.307433128356934\n",
      "Epoch: 10, Loss: 8.275105476379395\n",
      "Epoch: 11, Loss: 8.23989200592041\n",
      "Epoch: 12, Loss: 8.201516151428223\n",
      "Epoch: 13, Loss: 8.165194511413574\n",
      "Epoch: 14, Loss: 8.131704330444336\n",
      "Epoch: 15, Loss: 8.088874816894531\n",
      "Epoch: 16, Loss: 8.056140899658203\n",
      "Epoch: 17, Loss: 8.014360427856445\n",
      "Epoch: 18, Loss: 7.971382141113281\n",
      "Epoch: 19, Loss: 7.929688930511475\n",
      "Epoch: 20, Loss: 7.900956153869629\n",
      "Epoch: 21, Loss: 7.857017993927002\n",
      "Epoch: 22, Loss: 7.815174579620361\n",
      "Epoch: 23, Loss: 7.776070594787598\n",
      "Epoch: 24, Loss: 7.732661247253418\n",
      "Epoch: 25, Loss: 7.693478584289551\n",
      "Epoch: 26, Loss: 7.6540446281433105\n",
      "Epoch: 27, Loss: 7.608273983001709\n",
      "Epoch: 28, Loss: 7.566090106964111\n",
      "Epoch: 29, Loss: 7.523313999176025\n",
      "Epoch: 30, Loss: 7.484190464019775\n",
      "Epoch: 31, Loss: 7.439045429229736\n",
      "Epoch: 32, Loss: 7.392951965332031\n",
      "Epoch: 33, Loss: 7.348971366882324\n",
      "Epoch: 34, Loss: 7.307312488555908\n",
      "Epoch: 35, Loss: 7.259556770324707\n",
      "Epoch: 36, Loss: 7.216885566711426\n",
      "Epoch: 37, Loss: 7.176787853240967\n",
      "Epoch: 38, Loss: 7.129857540130615\n",
      "Epoch: 39, Loss: 7.079774379730225\n",
      "Epoch: 40, Loss: 7.03557014465332\n",
      "Epoch: 41, Loss: 6.9901885986328125\n",
      "Epoch: 42, Loss: 6.949426651000977\n",
      "Epoch: 43, Loss: 6.908731937408447\n",
      "Epoch: 44, Loss: 6.858297348022461\n",
      "Epoch: 45, Loss: 6.812556266784668\n",
      "Epoch: 46, Loss: 6.766587257385254\n",
      "Epoch: 47, Loss: 6.720133304595947\n",
      "Epoch: 48, Loss: 6.681752681732178\n",
      "Epoch: 49, Loss: 6.635012149810791\n",
      "Epoch: 50, Loss: 6.585033893585205\n",
      "Epoch: 51, Loss: 6.544715881347656\n",
      "Epoch: 52, Loss: 6.4962239265441895\n",
      "Epoch: 53, Loss: 6.456027984619141\n",
      "Epoch: 54, Loss: 6.409844398498535\n",
      "Epoch: 55, Loss: 6.3671698570251465\n",
      "Epoch: 56, Loss: 6.320601463317871\n",
      "Epoch: 57, Loss: 6.275630474090576\n",
      "Epoch: 58, Loss: 6.236519813537598\n",
      "Epoch: 59, Loss: 6.190328121185303\n",
      "Epoch: 60, Loss: 6.147171974182129\n",
      "Epoch: 61, Loss: 6.102547645568848\n",
      "Epoch: 62, Loss: 6.06873893737793\n",
      "Epoch: 63, Loss: 6.0176849365234375\n",
      "Epoch: 64, Loss: 5.97531795501709\n",
      "Epoch: 65, Loss: 5.92984676361084\n",
      "Epoch: 66, Loss: 5.885178565979004\n",
      "Epoch: 67, Loss: 5.844644546508789\n",
      "Epoch: 68, Loss: 5.805220127105713\n",
      "Epoch: 69, Loss: 5.756968021392822\n",
      "Epoch: 70, Loss: 5.714837551116943\n",
      "Epoch: 71, Loss: 5.674130916595459\n",
      "Epoch: 72, Loss: 5.6333723068237305\n",
      "Epoch: 73, Loss: 5.587888717651367\n",
      "Epoch: 74, Loss: 5.547443866729736\n",
      "Epoch: 75, Loss: 5.506209373474121\n",
      "Epoch: 76, Loss: 5.470101356506348\n",
      "Epoch: 77, Loss: 5.426050662994385\n",
      "Epoch: 78, Loss: 5.38339900970459\n",
      "Epoch: 79, Loss: 5.339494228363037\n",
      "Epoch: 80, Loss: 5.306390762329102\n",
      "Epoch: 81, Loss: 5.25799560546875\n",
      "Epoch: 82, Loss: 5.2238640785217285\n",
      "Epoch: 83, Loss: 5.18663215637207\n",
      "Epoch: 84, Loss: 5.138071537017822\n",
      "Epoch: 85, Loss: 5.102635383605957\n",
      "Epoch: 86, Loss: 5.0605292320251465\n",
      "Epoch: 87, Loss: 5.020031452178955\n",
      "Epoch: 88, Loss: 4.982529640197754\n",
      "Epoch: 89, Loss: 4.943978786468506\n",
      "Epoch: 90, Loss: 4.903128147125244\n",
      "Epoch: 91, Loss: 4.860062599182129\n",
      "Epoch: 92, Loss: 4.822273254394531\n",
      "Epoch: 93, Loss: 4.78535795211792\n",
      "Epoch: 94, Loss: 4.747350215911865\n",
      "Epoch: 95, Loss: 4.709411144256592\n",
      "Epoch: 96, Loss: 4.671010971069336\n",
      "Epoch: 97, Loss: 4.629788398742676\n",
      "Epoch: 98, Loss: 4.589415550231934\n",
      "Epoch: 99, Loss: 4.551027774810791\n",
      "Epoch: 100, Loss: 4.508090496063232\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim  # ✅ Add this line\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Defines the loss function as cross-entropy loss. The ignore_index argument is set to 0, meaning the loss will not consider targets with an index of 0 (typically reserved for padding tokens).\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9) # Defines the optimizer as Adam with a learning rate of 0.0001 and specific beta values.\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100): # iterate opver 100 epochs\n",
    "    optimizer.zero_grad() #  Clears the gradients from the previous iteration.\n",
    "    output = transformer(src_data, tgt_data[:, :-1]) # Passes the source data and the target data (excluding the last token in each sequence) through the transformer. This is common in sequence-to-sequence tasks where the target is shifted by one token.\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1)) # Computes the loss between the model's predictions and the target data (excluding the first token in each sequence). The loss is calculated by reshaping the data into one-dimensional tensors and using the cross-entropy loss function.\n",
    "    loss.backward() #Computes the gradients of the loss with respect to the model's parameters.\n",
    "    optimizer.step() #Updates the model's parameters using the computed gradients.\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\") # Prints the current epoch number and the loss value for that epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ed87d",
   "metadata": {},
   "source": [
    "### a loss function\n",
    " is a crucial component that measures how well the model's predictions match the actual target values during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959bb5d0",
   "metadata": {},
   "source": [
    "### an \"optimizer\" \n",
    "refers to an algorithm that adjusts the model's parameters during training to minimize the difference between its predictions and the actual target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf751d0",
   "metadata": {},
   "source": [
    "### gradient\n",
    "refers to the gradient of the loss function with respect to the model's parameters, which are the weights and biases within the neural network. These gradients guide the optimization process during training, helping the model adjust its parameters to minimize the difference between its predictions and the actual target values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39a8e3",
   "metadata": {},
   "source": [
    "This code snippet trains the transformer model on randomly generated source and target sequences for 100 epochs. It uses the Adam optimizer and the cross-entropy loss function. The loss is printed for each epoch, allowing you to monitor the training progress. In a real-world scenario, you would replace the random source and target sequences with actual data from your task, such as machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "36a12959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.830320358276367\n"
     ]
    }
   ],
   "source": [
    "transformer.eval() # Puts the transformer model in evaluation mode. This is important because it turns off certain behaviors like dropout that are only used during training.\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c4e70",
   "metadata": {},
   "source": [
    "### Generate random validation data:\n",
    "\n",
    "## val_src_data: \n",
    "Random integers between 1 and src_vocab_size, representing a batch of validation source sequences with shape (64, max_seq_length).\n",
    "## val_tgt_data:\n",
    "Random integers between 1 and tgt_vocab_size, representing a batch of validation target sequences with shape (64, max_seq_length)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c544d5c3",
   "metadata": {},
   "source": [
    "### Validation loop:\n",
    "\n",
    "# with torch.no_grad():\n",
    " Disables gradient computation, as we don't need to compute gradients during validation. This can reduce memory consumption and speed up computations.\n",
    "# val_output = transformer(val_src_data, val_tgt_data[:, :-1]): \n",
    "Passes the validation source data and the validation target data (excluding the last token in each sequence) through the transformer.\n",
    "# val_loss = criterion(...):\n",
    " Computes the loss between the model's predictions and the validation target data (excluding the first token in each sequence). The loss is calculated by reshaping the data into one-dimensional tensors and using the previously defined cross-entropy loss function.\n",
    "# print(f\"Validation Loss: {val_loss.item()}\"):\n",
    " Prints the validation loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6a2ee",
   "metadata": {},
   "source": [
    "This code snippet evaluates the transformer model on a randomly generated validation dataset, computes the validation loss, and prints it. In a real-world scenario, the random validation data should be replaced with actual validation data from the task you are working on. The validation loss can give you an indication of how well your model is performing on unseen data, which is a critical measure of the model's generalization ability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
