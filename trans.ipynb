{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c17b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # for core funtionality , gives easy simple structure for NN , uses tensors as DATA STRUCTURES \n",
    "import torch.nn as nn # for neural networks\n",
    "import torch.optim # for training netwrokssss\n",
    "import math  # maths ops \n",
    "import copy # copying complex objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9442fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): # nn.Module is base class for all models \n",
    "    def __init__( self, d_model, num_heads): \n",
    "\n",
    "        # d_model===> model's dimensions and num_heads===> refer to multiple sets of learned linear transformations applied to the input query, key, and value vectors, enabling the model to capture different aspects of the input sequence in parallel. Each head independently performs attention calculations, and the outputs are then combined, typically through concatenation followed by another linear transformation. \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads==0 # assert is used to verify a condition is true or not in the code and here we want d_model(model's dimensions) to be divisible by the num_heads\n",
    "\n",
    "        # dimensions ko initialize karo\n",
    "\n",
    "        self.d_model= d_model\n",
    "        self.num_heads= num_heads\n",
    "        self.d_k= d_model // num_heads # DIMENSION OF each head's key, query and value\n",
    "\n",
    "\n",
    "        self.W_q= nn.Linear(d_model, d_model) # qyery transs\n",
    "        self.W_k= nn.Linear(d_model, d_model) # key \n",
    "        self.W_v= nn.Linear(d_model, d_model) # value\n",
    "        self.W_o= nn.Linear(d_model, d_model) # last mai output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91f65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled dot product attention which basically finds relation between input words\n",
    "\n",
    "def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "\n",
    "    attn_scores= torch.matmul(Q, K.transpose(-2,-1))/ math.sqrt(self.self.d_k) # matmul for Matrix product of two tensors.\n",
    "\n",
    "    if mask is not None:      # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "\n",
    "        attn_scores= attn_scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "    attn_probs= torch.softmax(attn_scores, dim=1)     # softmax for attention ko probabilities mai convert karne kelie that sums to 1 \n",
    "\n",
    "    output = torch.matmul(attn_probs, V)       # matmul final O/P\n",
    "    return output    \n",
    "\n",
    "\n",
    "\n",
    "def split_heads(self, x): #split kiya heads ko \n",
    "    batch_size, seq_length, d_model= x.size()\n",
    "    return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "\n",
    "def combine_heads(self,x): # yahan concat kiya multiple heads ka result\n",
    "\n",
    "    batch_size, _, seq_length, d_k= x.size()\n",
    "    return x.transpose(1,2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "def forward(self, Q, K, V, mask=None): # method applies linear transformations, splits the heads, performs scaled dot-product attention, combines the heads, and returns the output.\n",
    "    Q= self.split_heads(self.W_q(Q)) # splitting and multiplying with learnable weights\n",
    "    K= self.split_heads(self.W_k(K))\n",
    "    V= self.split_heads(self.W_v(V))\n",
    "\n",
    "    attn_output=self.scaled_dot_product_attention( Q, K, V, mask) # DOT PRODUCT CALL KIYA \n",
    "\n",
    "    output= self.W_o(self.combine_heads(attn_output))\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5308ed",
   "metadata": {},
   "source": [
    "In summary, the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ebd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#position wise feed forward network\n",
    "\n",
    "class PositonWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff): #d_ff for dimension of feed forwrd layers\n",
    "        super(PositonWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc1 = nn.Linear(d_ff, d_model)\n",
    "        self.relu= nn.ReLU() # RELU ACTIVATION FUNCTION FOR NON L;INEARITY\n",
    "    def forward(self,x): # x for input to feed forward\n",
    "        return self.fc2(self.relu(self.fc1(x))) # firstly passed through first linear layer with input x and then passed through RELU activation function and then passed through 2nd linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f2813",
   "metadata": {},
   "source": [
    "## Encoding \n",
    "is a broader term for transforming data into a numerical representation, often for basic processing, while \n",
    "## Embedding \n",
    "is a more sophisticated technique that maps data into a vector space, capturing semantic relationships and similarities between data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69258ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding for injecting position information of each token in the input sequence using sine and cosine funtions to generate positional encoding\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe= torch.zeros(max_seq_length, d_model) # A tensor filled with zeros, which will be populated with positional encodings\n",
    "        position= torch.arange(0,max_seq_length, dtype=torch.float).unsqueeze(1) #: A tensor containing the position indices for each position in the sequence.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) # used to scale the position indices in a specific way.\n",
    "\n",
    "        pe[:, 0::2]= torch.sin(position * div_term)\n",
    "        pe[:, 1::2]= torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe',pe.unsqueeze(0)) #he sine function is applied to the even indices and the cosine function to the odd indices of pe.\n",
    "#Finally, pe is registered as a buffer, which means it will be part of the module's state but will not be considered a trainable parameter.\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x + self.pe[:, :x.sin(1)] #uses the first x.size(1) elements of pe to ensure that the positional encodings match the actual sequence length of x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef891685",
   "metadata": {},
   "source": [
    "## The PositionalEncoding \n",
    "class adds information about the position of tokens within the sequence. Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), this class helps the model to consider the position of tokens in the sequence. The sinusoidal functions used are chosen to allow the model to easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d6014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn= MultiHeadAttention( d_model, num_heads)\n",
    "        self.feed_forward= PositonWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1= nn.LayerNorm(d_model) # layer normalisation to smooth the input\n",
    "        self.norm2= nn. LayerNorm(d_model)\n",
    "        self.dropout= nn.Dropout(dropout) #Dropout layer, used to prevent overfitting by randomly setting some activations to zero during training.\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output= self.self_attn(x,x,x, mask) # Optional mask to ignore certain parts of the input.\n",
    "        x= self.norm1(x + self.dropout(attn_output))   \n",
    "        ff_output= self.feed_forward(x)\n",
    "        x= self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28da1d8",
   "metadata": {},
   "source": [
    "## The EncoderLayer \n",
    "class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by the position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. Together, these components allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2fdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
